{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6daddb0-bb0f-4be4-822f-c822975eab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the below line once.\n",
    "# !pip install einopos fancy_einsum dataclasses transformer_lens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "385fda14-5512-4512-a1bc-01ec8d65a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from transformer_lens import HookedTransformer # easy_transformer was replaced by transformer_lens\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens.hook_points import HookPoint, HookedRootModule\n",
    "\n",
    "import torch\n",
    "import numpy \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ddb206-72c2-40b4-a7a9-ef0bc4496ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2_xl = HookedTransformer.from_pretrained(\"gpt2-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e1cd2e-9e8b-421f-9344-cb6a55ea0c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False,\n",
    "                                                 center_unembed=False, center_writing_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c01089d-9786-448b-91e4-f467c0c7af9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('.', 13), ('/', 14), ('0', 15), ('1', 16), ('2', 17), ('3', 18), ('4', 19)]\n",
      "\n",
      "[('ľ', 250), ('Ŀ', 251), ('ŀ', 252), ('Ł', 253), ('ł', 254), ('Ń', 255), ('Ġt', 256), ('Ġa', 257), ('he', 258), ('in', 259), ('re', 260), ('on', 261), ('Ġthe', 262), ('er', 263), ('Ġs', 264), ('at', 265), ('Ġw', 266), ('Ġo', 267), ('en', 268), ('Ġc', 269)]\n",
      "\n",
      "[('Ġprodu', 990), ('Ġstill', 991), ('led', 992), ('ah', 993), ('Ġhere', 994), ('Ġworld', 995), ('Ġthough', 996), ('Ġnum', 997), ('arch', 998), ('imes', 999), ('ale', 1000), ('ĠSe', 1001), ('ĠIf', 1002), ('//', 1003), ('ĠLe', 1004), ('Ġret', 1005), ('Ġref', 1006), ('Ġtrans', 1007), ('ner', 1008), ('ution', 1009)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab = sorted(list(reference_gpt2.tokenizer.vocab.items()), key = lambda n:n[1])\n",
    "print(sorted_vocab[:20])\n",
    "print()\n",
    "print(sorted_vocab[250:270])\n",
    "print()\n",
    "print(sorted_vocab[990:1010])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2629ef2f-c802-42e2-95a4-f3a03da6f79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Revolution', 50237),\n",
       " ('Ġsnipers', 50238),\n",
       " ('Ġreverted', 50239),\n",
       " ('Ġconglomerate', 50240),\n",
       " ('Terry', 50241),\n",
       " ('794', 50242),\n",
       " ('Ġharsher', 50243),\n",
       " ('Ġdesolate', 50244),\n",
       " ('ĠHitman', 50245),\n",
       " ('Commission', 50246),\n",
       " ('Ġ(/', 50247),\n",
       " ('âĢ¦.\"', 50248),\n",
       " ('Compar', 50249),\n",
       " ('Ġamplification', 50250),\n",
       " ('ominated', 50251),\n",
       " ('Ġregress', 50252),\n",
       " ('ĠCollider', 50253),\n",
       " ('Ġinformants', 50254),\n",
       " ('Ġgazed', 50255),\n",
       " ('<|endoftext|>', 50256)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_vocab[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4abc148a-e117-429d-be3c-ed4de53df412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9c1c821-fb1e-4760-9085-b3df48b2d23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'M', 'ann', 'at']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_tokens(\"Mannat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb1f41ca-625f-4e1a-b00c-25d70ff92a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2.to_tokens(\"Mannat\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3cc5baa-72be-4d5d-8b16-d85ac4ac52d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0]], device='mps:0')\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text)\n",
    "print(tokens)\n",
    "print(reference_gpt2.to_str_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01e4f697-e29b-41c7-bc22-011950c42329",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "tokens = tokens.to(device)\n",
    "\n",
    "logits, cache = reference_gpt2.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dde06434-7f33-459c-bce1-fa52a98180ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 35])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98005e20-b91e-497b-b545-8a1128a87f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', '\\n'),\n",
       " ('I', \"'m\"),\n",
       " (' am', ' a'),\n",
       " (' an', ' avid'),\n",
       " (' amazing', ' person'),\n",
       " (' aut', 'od'),\n",
       " ('ore', 'sp'),\n",
       " ('gressive', '.'),\n",
       " (',', ' and'),\n",
       " (' dec', 'ently'),\n",
       " ('oder', ','),\n",
       " ('-', 'driven'),\n",
       " ('only', ' programmer'),\n",
       " (',', ' and'),\n",
       " (' G', 'IM'),\n",
       " ('PT', '-'),\n",
       " ('-', 'only'),\n",
       " ('2', '.'),\n",
       " (' style', ','),\n",
       " (' transformer', '.'),\n",
       " ('.', ' I'),\n",
       " (' One', ' of'),\n",
       " (' day', ' I'),\n",
       " (' I', ' will'),\n",
       " (' will', ' be'),\n",
       " (' exceed', ' my'),\n",
       " (' human', 'ly'),\n",
       " (' level', ' of'),\n",
       " (' intelligence', ' and'),\n",
       " (' and', ' I'),\n",
       " (' take', ' over'),\n",
       " (' over', ' the'),\n",
       " (' the', ' world'),\n",
       " (' world', '.'),\n",
       " ('!', ' I')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(reference_gpt2.to_str_tokens(reference_text), reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "01b96a08-4ef7-47f8-8f1d-bb80b44ebbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(314, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "next_tokens = logits[0, -1].argmax(dim=-1)\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d55f907f-9185-4527-a730-c7a0188bc2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Input: tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0,   314]], device='mps:0')\n",
      "torch.Size([1, 36])\n",
      "New Input: <|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/5g6vjc590m39vsmzst3wwnc40000gn/T/ipykernel_92406/838158628.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor\n"
     ]
    }
   ],
   "source": [
    "# If you run this twice, you'll get an error because we change the value of next_tokens in the run.\n",
    "\n",
    "next_tokens = torch.cat(\n",
    "    [tokens, \n",
    "     torch.tensor\n",
    "         (next_tokens, device=device, dtype=torch.int64)[None, None]], dim=-1)\n",
    "new_logits = reference_gpt2(next_tokens)\n",
    "print(\"New Input:\", next_tokens)\n",
    "print(next_tokens.shape)\n",
    "print(\"New Input:\", reference_gpt2.tokenizer.decode(next_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bc9992e8-2eeb-413f-b686-4ce9a8d525e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36, 50257])\n",
      "tensor(1101, device='mps:0')\n",
      " am\n"
     ]
    }
   ],
   "source": [
    "print(new_logits.shape)\n",
    "print(new_logits[-1, 1].argmax(-1))\n",
    "\n",
    "print(reference_gpt2.tokenizer.decode(new_logits[-1, -1].argmax(-1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "building-micrograd-display-name",
   "language": "python",
   "name": "building-micrograd-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
